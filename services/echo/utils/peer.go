// ancillary funcions appertain to webrtc (pion implementaion)
// credits go to https://github.com/pion/webrtc/blob/master/examples/broadcast/main.go#L229
package utils

import (
	"encoding/json"
	"errors"
	"io"
	"log"

	"echo/state"

	"github.com/pion/interceptor"
	"github.com/pion/interceptor/pkg/intervalpli"
	"github.com/pion/webrtc/v4"
)

/*
establish a webrtc peer connection by using pion webrtc. this function does
more than establishing the connection; it manages the global state of peers
connections and streams as well.

Note: it returns the peer connection with the associated id if it's already
established before, by retrieving it from the state package.
*/
func GetPeerConn(id int, role state.PeerRole, config webrtc.Configuration) (*webrtc.PeerConnection, error) {
	// check if the peer connection is already established
	if state.Get(id, role) != nil {
		return state.Get(id, role).Conn, nil
	}

	mediaEngine := &webrtc.MediaEngine{}
	if err := mediaEngine.RegisterDefaultCodecs(); err != nil {
		panic(err)
	}

	// Create a InterceptorRegistry. This is the user configurable RTP/RTCP Pipeline.
	// This provides NACKs, RTCP Reports and other features. If you use `webrtc.NewPeerConnection`
	// this is enabled by default. If you are manually managing You MUST create a InterceptorRegistry
	// for each PeerConnection.
	interceptorRegistry := &interceptor.Registry{}

	// Use the default set of Interceptors
	if err := webrtc.RegisterDefaultInterceptors(mediaEngine, interceptorRegistry); err != nil {
		return nil, err
	}

	// Register a intervalpli factory
	// This interceptor sends a PLI every 3 seconds. A PLI causes a video keyframe to be generated by the sender.
	// This makes our video seekable and more error resilent, but at a cost of lower picture quality and higher bitrates
	// TODO: A real world application should process incoming RTCP packets from viewers and forward them to senders
	intervalPliFactory, err := intervalpli.NewReceiverInterceptor()
	if err != nil {
		return nil, err
	}
	interceptorRegistry.Add(intervalPliFactory)

	// Create a new RTCPeerConnection
	conn, err := webrtc.NewAPI(
		webrtc.WithMediaEngine(mediaEngine),
		webrtc.WithInterceptorRegistry(interceptorRegistry),
	).NewPeerConnection(config)

	// ensure that the peer connection only stored when it's connected
	// and moreover it's removed when the connection is terminated
	conn.OnConnectionStateChange(func(connState webrtc.PeerConnectionState) {
		log.Printf("Peer %d: %s (%s)", id, connState, state.PeerRoleProducer)
		if connState == webrtc.PeerConnectionStateConnected {
			state.Add(id, role, conn)
		} else {
			state.Remove(id, role)
		}
	})

	// store different tracks received from the remote peer in the global state
	conn.OnTrack(func(remoteTrack *webrtc.TrackRemote, _ *webrtc.RTPReceiver) {

		log.Printf("Track received from peer %d: kind: %s, codec: %s", id, remoteTrack.Kind().String(), remoteTrack.Codec().MimeType)

		// create a local track with the remote track capabilities
		localTrack, newTrackErr := webrtc.NewTrackLocalStaticRTP(remoteTrack.Codec().RTPCodecCapability, remoteTrack.ID(), remoteTrack.ID())
		if newTrackErr != nil {
			panic(newTrackErr)
		}

		// store the track address in the state map
		peer := state.Get(id, role)
		if peer == nil {
			log.Println("container not found! should never happen.")
			return
		}
		peer.Tracks = append(peer.Tracks, localTrack)

		// write the buffer from the remote track in the local track simultaneously
		rtpBuf := make([]byte, 1400)
		for {
			i, _, readErr := remoteTrack.Read(rtpBuf)
			if readErr != nil {
				log.Println(readErr)
				break
			}
			// ErrClosedPipe means we don't have any subscribers, this is ok if no peers have connected yet
			if _, err = localTrack.Write(rtpBuf[:i]); err != nil && !errors.Is(err, io.ErrClosedPipe) {
				log.Println(err)
				break
			}
		}
	})

	// TODO: move data channels to the state map
	var dataChannel *webrtc.DataChannel

	conn.OnDataChannel(func(dc *webrtc.DataChannel) {
		dc.OnOpen(func() {
			dataChannel = dc
			dc.OnMessage(func(msg webrtc.DataChannelMessage) {
				ice := &webrtc.ICECandidateInit{}
				if err := json.Unmarshal(msg.Data, ice); err != nil {
					log.Println("error:", err)
				}
				if err := conn.AddICECandidate(*ice); err != nil {
					log.Println("warning: candidate couldn't be added.")
				}
			})
		})
	})

	// When Pion gathers a new ICE Candidate send it to the client. This is how
	// ice trickle is implemented. Everytime we have a new candidate available we send
	// it as soon as it is ready. We don't wait to emit a Offer/Answer until they are
	// all available
	conn.OnICECandidate(func(candidate *webrtc.ICECandidate) {
		if candidate == nil {
			return
		}
		for dataChannel == nil {
		} // block until the data channel is open
		iceInit, err := json.Marshal(candidate.ToJSON())
		if err != nil {
			log.Println("error:", err)
			return
		}
		dataChannel.SendText(string(iceInit))
	})

	return conn, err
}

/*
close an already established peer connection. an error is returned
if the connection cannot be closed. and nothing happens if the peer
connection cannot be found.
*/
func ClosePeerConn(id int, role state.PeerRole) error {
	peer := state.Get(id, role)
	if peer == nil {
		return nil
	}
	return peer.Conn.Close()
}
